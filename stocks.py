import logging
import os
import random
import re
import sys
import time
from datetime import datetime, timedelta, timezone
from email.utils import format_datetime

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from feedgen.feed import FeedGenerator

# é¦™æ¸¯æ™‚å€ï¼šUTC+8
hk_tz = timezone(timedelta(hours=8))

MAX_RETRIES = 5

load_dotenv()

# è®€å–ç’°å¢ƒè®Šæ•¸ DEBUGï¼Œå¦‚æœç„¡å°±é è¨­ç‚º False
DEBUG = os.getenv("DEBUG", "").lower() == "true"

# è¨­å®š logging format åŒ level
logging.basicConfig(
    level=logging.INFO if DEBUG else logging.WARNING,
    format='%(asctime)s - %(levelname)s - %(message)s',
    force=True  # Python 3.8+ï¼Œç¢ºä¿å””æœƒè¢«å…¶ä»– handler override
)


def parse_stock_list(env_var):
    raw = os.getenv(env_var, "")
    return [s.strip() for s in raw.split(",") if s.strip()]


def fetch_news(stock_code):
    # ğŸ§© æ“·å–æ–°èè³‡æ–™
    url = f"https://www.aastocks.com/tc/stocks/analysis/stock-aafn/{stock_code}/0/hk-stock-news/1"
    headers = {"User-Agent": "Mozilla/5.0"}
    retries = 0

    while retries < MAX_RETRIES:
        try:
            resp = requests.get(url, headers=headers, timeout=10)
            resp.raise_for_status()  # å¦‚æœ status code >= 400 å°± raise error
            logging.info(f"âœ… æˆåŠŸæ“·å– {stock_code} çš„æ–°è")
            soup = BeautifulSoup(resp.text, "html.parser")

            content_div = soup.find(
                "div", class_="content", id="aafn-search-c1")
            news_blocks = content_div.find_all("div", attrs={"ref": True})

            news_list = []
            for block in news_blocks:
                title_tag = block.select_one(".newshead4 a")
                date_tag = block.select_one(".newstime4 .inline_block")
                summary_tag = block.select_one(".newscontent4")
                img_tag = block.select_one(".newsImage4a img")

                title = title_tag.text.strip() if title_tag else "ï¼ˆç„¡æ¨™é¡Œï¼‰"
                link = title_tag["href"] if title_tag else "#"
                if not link.startswith("http"):
                    link = "https://www.aastocks.com" + link

                # æ“·å–æ™‚é–“ï¼ˆå¾ script ä¸­æŠ½å‡º dtï¼‰
                try:
                    script_text = date_tag.find("script").string
                    match = re.search(r"dt:'([\d/:\s]+)'", script_text)
                    raw_date = match.group(1) if match else None
                    dt = datetime.strptime(
                        raw_date, "%Y/%m/%d %H:%M").replace(tzinfo=hk_tz)
                    pub_date = format_datetime(dt)
                except:
                    pub_date = format_datetime(datetime.now())

                summary = summary_tag.text.strip() if summary_tag else title
                img_url = img_tag["src"] if img_tag and img_tag.has_attr(
                    "src") else None
                ref_id = block.get("ref")

                news_list.append({
                    "title": title,
                    "link": link,
                    "pubDate": pub_date,
                    "summary": summary,
                    "image": img_url,
                    "guid": ref_id
                })

            return news_list
        except requests.exceptions.RequestException as e:
            retries += 1
            logging.info(f"âš ï¸ æ“·å– {stock_code} å¤±æ•—ï¼ˆç¬¬ {retries} æ¬¡ï¼‰ï¼ŒéŒ¯èª¤ï¼š{e}")
            time.sleep(2 ** retries)  # æŒ‡æ•¸å¼å»¶é²ï¼š2s, 4s, 8s

    logging.error(f"âŒ æœ€å¤šé‡è©¦ {MAX_RETRIES} æ¬¡ï¼Œæ”¾æ£„ {stock_code}")
    return []


def generate_rss(all_news, output_filename="combined_stock_news.xml"):
    fg = FeedGenerator()
    fg.title("AASTOCKS ç¶œåˆè‚¡ç¥¨æ–°è")
    fg.link(href="https://www.aastocks.com/tc/stocks/news/aafn")
    fg.description("ç¶œåˆå¤šéš»è‚¡ç¥¨çš„æœ€æ–°æ–°è RSS Feed")
    fg.language("zh-HK")
    fg.lastBuildDate(datetime.now(hk_tz))

    for news in all_news:
        fe = fg.add_entry()
        fe.title(news["title"])
        fe.link(href=news["link"])
        fe.guid(news["guid"], permalink=False)
        fe.pubDate(datetime.strptime(
            news["pubDate"], "%a, %d %b %Y %H:%M:%S %z"))
        fe.description(news["summary"])
        if news["image"]:
            fe.enclosure(news["image"], 0, "image/jpeg")

    output_path = os.path.join(os.path.dirname(__file__), output_filename)
    fg.rss_file(output_path)
    logging.info(f"âœ… RSS feed å·²å„²å­˜ï¼š{output_filename}")


def process_feed(stock_list, output_filename):
    all_news = []
    for stock_code in stock_list:
        logging.info(f"ğŸ“¡ æ“·å– {stock_code} çš„æ–°èä¸­...")
        news = fetch_news(stock_code)
        all_news.extend(news)
        time.sleep(random.uniform(2, 5))  # å®‰å…¨å»¶é²
    generate_rss(all_news, output_filename)

# def read_stock_list(file_path):
#     with open(file_path, "r", encoding="utf-8") as f:
#         return [line.strip() for line in f if line.strip()]


def main():
    # ğŸ§ª åŸ·è¡Œ
    # if len(sys.argv) < 2:
    #     print("âŒ è«‹æä¾›è‚¡ç¥¨æ¸…å–® txt æª”æ¡ˆï¼Œä¾‹å¦‚ï¼špython gen_rss.py my_stocks.txt")
    #     return

    # file_path = sys.argv[1]
    # stock_list = read_stock_list(file_path)

    my_stocks = parse_stock_list("STOCK_LIST_MY")
    watchlist = parse_stock_list("STOCK_LIST_WATCH")

    process_feed(my_stocks, "stocks_rss.xml")
    process_feed(watchlist, "watch_rss.xml")


if __name__ == "__main__":
    main()
